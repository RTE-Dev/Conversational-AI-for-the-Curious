## 5.2 LLM——让 AI 能够思考
在一个完整的对话链路中，ASR 将语音转化为文本，而 LLM 则负责“理解”与“决策”。

在 对话式 AI 的语境下，LLM 不只是一个文本生成器，而是整个系统的“大脑”：

- 它需要在毫秒级时间内理解上下文意图；
- 它要决定是否调用 Function Call（执行任务）；
- 它要为 TTS 输出生成连贯、自然的语言。

换句话说，ASR 听懂“你说了什么”，而 LLM 要理解“你真正想要什么”。一个高质量的 对话式 AI，往往不是依赖最强大的模型，而是依赖**最合适的模型结构**。

### 5.2.1 速度与思考的平衡：选择哪种 LLM？

在 对话式 AI 的场景中，延迟是一切体验的关键。开发者在选择 LLM 时，往往要在以下两种模型之间做出权衡：

|   |   |   |   |
|---|---|---|---|
|模型类型|优点|缺点|适用场景|
|深度推理型（如 GPT-4、Claude 3）|逻辑性强，回答准确，能理解长上下文|推理时间长，响应延迟高|金融顾问、智能客服|
|轻量快速型（如 GPT-4o-mini、Mistral、Gemma-2B）|响应极快，交互流畅|语言表达可能不够精准|实时语音助手、语音陪练、机器人对话|

对话式 AI 对延迟极为敏感——一个延迟 2 秒的响应在人机对话中会被感知为“思考太久”，而 1 秒内的反应则更接近人类自然交流。因此，**在实际部署中，大多数团队会采用“轻量模型 + 外部知识增强”的结构**，例如：

- 使用小模型作为主推理引擎
- 结合 RAG（检索增强）提供领域知识
- 结合 Memory（记忆模块）保证上下文连贯
- 结合 Function Call 实现动作决策

### 5.2.2 让 LLM 更聪明：Function Call、RAG 与 Memory
    

对话式 AI 不是孤立的语言模型，而是一个“带手带脑”的智能体。LLM 的核心作用不仅在于回答问题，还在于决定“**我该不该行动**”。

例如，当用户说：“帮我订一张明天去北京的机票”，LLM 并不会直接生成文字，而是触发一个 **Function Call**：

```JSON
{
  "function": "book_flight",
  "arguments": {
    "destination": "北京",
    "date": "明天"
  }
}
```

这类结构化调用让 对话式 AI 能够“说”与“做”同时发生。

在此之上，RAG（Retrieval-Augmented Generation）与 Memory 提供了“短期记忆”和“长期记忆”：

- **RAG**：通过检索外部文档、知识库或 API，为 LLM 提供上下文参考
- **Memory**：记录用户的个人偏好、历史任务与上下文状态

这意味着下一次你与 对话式 AI 对话时，它能记得你上次喜欢喝哪种咖啡。

由于 Function Call、RAG、Memory 的内容都会相对复杂，在下一章中，我们会进行详细分享。

### 5.2.3 成本、延迟与模型架构的取舍
    

正如 ASR 部分所言，在实际生产过程中，模型的价格也会极大地影响开发者的选择。在语音连续交互场景下，LLM 调用频率高、上下文长，费用极易翻倍。这就是为什么许多团队倾向于：

- 在云端使用小型模型（如 Llama3-8B、Qwen2.5-7B）
- 通过 RAG、Memory 扩展智能深度
- 在边缘设备上部署低延迟的轻量版本

一些开源替代方案（如 Qwen、Llama、Mistral、DeepSeek等）已经在社区中兴起，成为控制成本与保持体验之间的平衡点。