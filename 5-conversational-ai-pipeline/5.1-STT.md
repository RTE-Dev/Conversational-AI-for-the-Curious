## 5.1 STT ——让 AI “听懂人类语言” {#5-1-stt}
在文本聊天中，用户输入的文字天然具备结构化边界：你敲下回车，模型就知道输入结束了。但语音交互不同。用户并不会告诉系统“我说完了”，也不会等系统准备好才开口说。

因此，一个 对话式 AI 的第一个核心任务，就是**听得懂**。自动语音识别ASR（Automatic Speech Recognition）负责把声音转成文字，它是整个系统的“耳朵”。

### 5.1.1 为什么关注“流式识别”（Streaming ASR） {#5-1-1-streaming-asr}
    

在传统的非流式 ASR 系统中，模型需要完整地听完一句话、等待静音结束后，才能输出结果。这类系统适合录音转写、字幕生成等“后处理”场景，但不适合语音交互。

对话式 AI 则完全不同。用户希望 AI 能**边听边理解、边想边回应**。如果系统等到整句话结束再开始识别，延迟往往超过 1 秒，这种“滞后感”会让对话显得机械。

这个时候，我们就需要流式 ASR （Streaming ASR），让模型在语音输入的同时开始输出文字。举个例子：

> 用户：“帮我查一下明天……的天气”
> 
> 流式 ASR 会在听到“帮我查”时就输出部分识别结果，
> 
> LLM 可以在后台提前预热上下文，当用户说完“明天”时几乎立即返回天气信息。

这就是流式识别的意义：**它不只是快，而是让对话变得自然。**

### 5.1.2 流式 ASR 的工作方式 {#5-1-2-streaming-asr-workflow}

流式识别的关键是“增量推理”与“结果修正”。

1. **增量推理（Partial Recognition）**
    

模型会实时输出当前帧的预测，比如每 100ms 更新一次识别结果。这让 对话式 AI 能边听边想，显著降低响应延迟。

2. **结果修正（Stabilization）**
    

为了保证准确率，模型会在收到更多上下文后不断修正前面的结果。比如刚开始识别成“我爱北京的天”，但在下一秒模型意识到应为“我爱北京的天气”，系统会自动更新。

流式模型的难点就在于这种**准确性与延迟的平衡**：推得太快，错误多；推得太慢，反应迟钝。

### 5.1.3 小语种与方言：实际落地的挑战 {#5-1-3-challenges}

在 对话式 AI 实践中，开发者常常遇到这样的问题：

模型在普通话或英语下表现很好，但一旦切换到方言、小语种或中英混说场景，就容易“听不懂”。

这是因为：

- 在粤语或日语混合英语的场景中，ASR 容易在词边界出现混淆；
    
- 东南亚地区的英语（如菲律宾口音、印式英语）会显著增加误判率；
    
- 对于低资源语言（如藏语、老挝语等），缺乏训练语料常导致模型“听不懂”。
    

常见的解决方式包括：

1. **多通道识别（Multi-ASR）**：同时调用不同语种模型，让系统根据置信度自动选择最佳结果。
    
2. **自定义热词（Custom Vocabulary）**：提升特定领域词汇的识别准确率（如“华为Mate 70”）。
    
3. **本地模型微调（Fine-tuning）**：对方言或行业数据进行再训练，以补足主模型能力。
    

### 5.1.4 成本与选择：开源模型的现实价值 {#5-1-4-how-to-choose}
    

对于实时语音交互场景中，约有80%的 对话式 AI 会选择流式架构。但因为它处理的是连续音频流，调用频率高、计算量大，成本也相对比较高。因此，许多开发者会在**商业 ASR API 与开源框架**之间寻找平衡：

|类型|代表方案|优点|局限|
|---|---|---|---|
|商业API|Google Cloud Speech、Azure、iFlyTek等|高准确率，多语支持，维护省心|成本高，定制能力有限|
|开源框架|Whisper、Kaldi2、Vosk 等|成本低，可本地化部署，可自定义|需要GPU资源与模型调优|

对于需要大规模部署的系统，一个常见策略是：

> “轻量模型 + 缓存 + 增量识别 + RAG/Mem融合”。

即：先用轻模型实时识别 + 语义缓存结果，后续再通过 Memory 或 RAG 校正错误识别。这种组合方式既降低了延迟，又避免了高昂的 API 费用。