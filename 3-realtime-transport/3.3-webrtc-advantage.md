## 3.3 为什么 WebRTC 更适合 对话式 AI  {#3-3-webrtc-advantage}

WebRTC 的优势不仅在“速度”，而在于**它把语音交互的复杂性封装在协议里**。 下面从四个维度解释它为何是语音代理的标准答案。

### 3.3.1 传输机制与实时性：让声音“现在发生”  {#3-3-1-transport-real-time}
    

语音 AI 的目标并不是“零延迟”，而是让交互听起来自然、顺畅。

大量实测显示：**800ms –1.5 秒** 的响应延迟是会被认为是“自然舒适”的。但若超过 **2 秒**，对话开始出现不连贯感；超过 **3 秒**，用户会误以为系统卡死或失效。

WebSocket 基于 TCP，所有包都必须按序送达。任何一个音频包的延迟或丢失，都会触发重传与阻塞（**Head-of-Line Blocking**），后续所有数据都会被拖慢。在弱网或移动网络下，这会造成**端到端延迟轻松突破 2 秒**，有时甚至达到 3 秒以上。这不仅让对话节奏变慢，还让语音交互失去“呼吸感”。

WebRTC 走 UDP 路线。它允许按需丢弃过期帧，优先传递最新音频，并自动做前向纠错（FEC）与码率调节。在网络波动 10–15% 的情况下，它仍能把整体响应控制在 1–1.5 秒。

**体验上，WebSocket 等于“等到完美”，WebRTC 等于“立刻回应”。**

### 3.3.2 实时控制与打断语义：像人一样“插话”  {#3-3-2-interruptions-and-control}
    

人类的对话不是线性的。我们经常在对方说话时插话、回应、打断。要做到这种“自然轮换”，系统必须能在毫秒级控制媒体流。

WebSocket 只是数据通道，不理解音频帧或时序；开发者需要手动实现“打断协议”，包括：停止播放、取消生成、清空缓存、恢复监听等。这一整套逻辑复杂又容易延迟。

WebRTC 则天生支持流式播放与 RTP 时间戳控制，可以让播放与采集线程独立运行。当检测到新语音输入（通过 VAD/轮次检测），可以立即停止当前播放，切换到收听状态，整个过程无感。**这种“边说边听”的能力，正是语音代理与普通聊天机器人的分水岭。**

### 3.3.3 设备与信号处理：从“听得清”开始  {#3-3-3-device-signal-processing}
    

真实设备的输入并不干净。回声、风噪、键盘声、麦克风距离，都会影响识别质量。

WebRTC 自带完整的 3A 处理链：AEC（回声消除）、ANS（降噪）、AGC（自动增益控制），能在底层完成音频修复，让送入 ASR 的信号更稳定、更可识别。

WebSocket 路线没有这些能力。开发者必须自行集成第三方 DSP 库、处理多平台兼容与性能问题。结果往往是：**你为了补全音质，手动造了一个迷你 WebRTC。**

### 3.3.4 可观测性与可维护性：看得见，才调得好  {#3-3-4-observability-maintainability}
    

当延迟或音质问题出现时，能否快速定位，是系统是否可维护的关键。

WebRTC 内置完整的 getStats() 接口与媒体统计系统，实时上报 RTT、丢包、抖动、码率、音量电平、缓冲深度等指标。开发者可以从端到端精确判断延迟发生在哪个环节。

WebSocket 只是一条黑箱字节流。要追踪类似指标，必须在应用层人工埋点、对齐时间戳、分析日志，成本高且容易误判。

**WebRTC 把“诊断能力”作为协议一部分，而 WebSocket 完全依赖应用自建。**